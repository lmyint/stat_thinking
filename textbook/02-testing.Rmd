# Hypothesis Testing {#chap_hypo_test}

In this chapter, we will learn about statistical hypothesis testing, one of the most ubiquitous frameworks for making statistical inferences - for learning about the world. We will start by learning the general conceptual framework underlying hypothesis tests. This will allow us to understand a large variety of statistical tests that are commonly used in scientific practice.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

## Aside: what is statistical inference?

I have said that hypothesis testing is a framework for making statistical inferences. What exactly is statistical inference? Statistical inference refers to the process of using data to make conclusions about the world. It deals with estimating true underlying quantities and expressing our uncertainty about those estimates. For example, we may assume that there is some true prevalence of malaria in a particular population which is unknown to us because we cannot collect malaria outcome data on everyone in the population. Instead, we may collect data on malaria outcomes within a certain research center. Statistical inference deals with using that collected data to estimate the true prevalence of malaria and to quantify our uncertainty about that estimate, typically with a range. This range is most often a confidence interval, a concept we will review in this chapter.

Hypothesis testing, we will see, is useful for comparing groups and is most often used to identify where there are differences between groups. This is a form of statistical inference because we are still using data to learn a truth about the world - the truth being whether or not these groups of interest are different. Differences are of fundamental interest in public health and in science because we care about comparisons: comparisons of different demographic groups, of policies, of old and new treatments.

```{block, type="respond"}
Think about a particular issue in public health that interests you. How would statistical inference play a role in understanding this issue? Specifically, what truth are you interested in estimating and why would quantification of the uncertainty of that estimate be useful?
```

## Conceptual framework

A statistical test aims to answer the question: is there a difference? This question can be made more specific in different situations:

- Is there a difference in the levels of a trait between groups? e.g. CD4 levels in different HIV risk groups.
- Is there a difference in the proportion of a trait between groups? e.g. Birth defects in the children of Zika-exposed and unexposed mothers.
- Is there a difference between the average level of a trait in population X and a meaningful cutoff value? e.g. Rate of operating room mortalities in a certain hospital as compared to the national average.

All of the above questions are common in public health practice and can be examined in the hypothesis testing framework. Let's develop these ideas using the example of adverse pregnancy outcomes in the children of Zika-exposed and unexposed mothers. In this case, the true underlying quantities of interest are (1) the probability of adverse pregnancy outcomes in Zika-exposed mothers $p_E$ and (2) the probability of adverse pregnancy outcomes in Zika-unexposed mothers $p_U$. $p_E$ and $p_U$ are also referred to as **parameters**.

The first component of a hypothesis test is the **null hypothesis**, denoted $H_0$. This is a statement about the true parameters that describes the situation where *nothing interesting is happening*. It is interesting if there is a difference in the rate of birth defects between Zika-exposed and Zika-unexposed mothers. What would be uninteresting? If there were no difference in the rate of birth defects between the two groups. In other words, it would be uninteresting if the true probabilities of birth defects are the same. We can write this mathematically as:

$$ H_0: p_E - p_U = 0 $$

Hypothesis testing is akin to proof by contradiction. In a proof by contradiction, we assume that the opposite of what we wish to prove is true. Under this assumption, we determine what must logically follow. If we end up with a statement that is false, then our original assumption must have been wrong. For example, let's say I want to prove the statement "Not all umbrellas are green." In a proof by contradiction, I would assume the opposite: "All umbrellas are green." What logically follows is that every umbrella I see must be green. I'm likely to stumble upon a counterexample very quickly, which proves my original assertion.

Hypothesis testing works similarly. We set up an assumption, the null hypothesis. Under this assumption, we use the tools of probability to determine how likely our data is. If our data is unlikely under this assumption, then perhaps our assumption was wrong to begin with. The key idea is that we set up our assumption to be a description of the world with nothing interesting going on. If this assumption might be wrong, then perhaps there *is* something interesting going on. This uncertainty is what differentiates hypothesis testing from the proof by contradiction. The moral of statistics, and perhaps science in general, is that few things can ever be proven without a doubt but that we can get close with accumulation of evidence.

The table below summarizes the comparison between proof by contradiction and hypothesis testing.

----------------------------------------------------------------------------------------------------
Step                 Proof by contradiction           Hypothesis testing
-------------------- -------------------------------- ----------------------------------------------
State what you       Not all umbrellas are green.     There is a difference in the rate of birth 
want to show                                          defects between children of Zika-exposed 
                                                      and unexposed mothers.

Make an assumption   All umbrellas are green.         There is no difference in the rate of birth 
that is the opposite                                  defects between children of Zika-exposed and 
of what you want                                      unexposed mothers.
to show

Collect data         Obtain umbrellas and record       Collect information on birth outcomes for 
                     their colors.                     Zika-exposed and unexposed mothers.

Evaluate             A single counterexample of a      Use a statistical test that provides a 
discrepancies        non-green umbrella is enough to   discrepancy measure (often a p-value).
                     prove our original assertion.
----------------------------------------------------------------------------------------------------


## Statistical details

The previous section set up the big picture ideas of hypothesis testing. In this section, we will delve into more of the statistical details. Let's continue with our example of determining if there is a difference in the rate of adverse pregnancy outcomes in Zika-exposed and unexposed mothers. Recall that the probability of adverse pregnancy outcomes in Zika-exposed mothers is $p_E$ and $p_U$ for unexposed mothers. The null hypothesis (which we aim to gather data to refute) describes the situation where nothing interesting is happening:

$$ H_0: p_E - p_U = 0 $$

This states that the probabilities of birth defects are equal in both groups. Recall from the previous section that the next step is to use collected data to evaluate if it is discrepant with this null hypothesis. Let's look at the following data from a cohort study published in late 2016:

                               Zika-positive    Zika-negative    Total
------------------------------ ---------------- ---------------- -----------
Adverse pregnancy outcomes     58               7                65
No adverse pregnancy outcomes  67               54               121
Total                          125              61               186


Note that $p_E - p_U$ in the formulation of the null hypothesis above denotes the true difference in probabilities. We can obtain an **estimate** of this true value by subtracting the sample proportions:

$$ \frac{58}{125} - \frac{7}{61} = 0.35 $$

This represents a resonable guess from our data of the difference in adverse outcome rates. We want to compare this estimate to 0, also called the **null value**. The null value gives the value of true difference in probabilities if nothing interesting were going on. Can't we just use the difference between our estimate and the null value? This is a step in the right direction but how do we know if this difference is big? What if differences of 0.35 happen quite often just by chance? We need to take into account the uncertaintly/variability of the estimate (called the **standard error**) to see how much this difference exceeds what we might reasonably see by chance. The quantity that is computed in statistical tests that accounts for all of this information (the estimate, the standard error, and the null value) is called a **test statistic**. Often, but not always, a test statistic has the form

$$ \hbox{test statistic} = \frac{\hbox{estimate} - \hbox{null value}}{\hbox{standard error of estimate}} $$

What is common to all test statistics is that they are used to give a measure of discrepancy with the null hypothesis. When test statistics are large, we reject the null hypothesis. In this case, we would say that there is indeed a difference in rates of adverse pregnancy outcomes between the two groups of mothers. Let's look at the formula above to see why it makes sense that larger test statistics suggest a higher level of discrepancy with the null hypothesis. We see that the test statistic is large when two things happen: (1) the estimate is far from the null value and (2) the standard error of the estimate is low. When (1) happens, our data is telling us that the quantity that we're interested in is quite different from the null value. When (2) happens, our estimate is more reliable. So if our estimate of the truth is far from the null value *and* is reliable, there is some suggestion that we should reject the null hypothesis. Although not all test statistics have this form, they do all have the general use that large values correspond to decisions to reject the null hypothesis.

How large does a test statistic need to be to reject the null hypothesis? Is a threshold of 4 suitable? Statistical theory is able to help us here. It turns out that different thresholds lead to different error rates - we may incorrectly reject the null hypothesis when we should not or we may incorrectly fail to reject the null hypothesis when we should reject it. In other words, errors occur if we claim differences when there are none or fail to notice differences when they truly exist. With this setup, the following outcomes are possible:

------------------------------------------------
                $H_0$ true       $H_A$ true
--------------- ---------------- ---------------
Reject          False positive   True positive
                Type I error     Power
                $\alpha$         $\beta$
Fail to reject  True negative    False negative
                $1-\alpha$       Type II error
                                 $1-\beta$
------------------------------------------------

To understand why different thresholds on the test statistic affect error rates, we need to understand how test statistics vary from dataset to dataset. The estimate of the true quantity of interest ($p_E - p_U$) varies from dataset to dataset, and because it is used to compute the test statistic, the test statistic also varies from dataset to dataset. In other words, the test statistic comes from a distribution which might look something like this.

```{r fig.align="center"}
x <- seq(-5, 5, 0.01)
y <- dt(x, df = 10)
plot(x, y, type = "l", main = "Distribution of test statistic", xlab = "Test statistic", ylab = "Density")
```

Note that this distribution is centered around zero. This turns out to be a good description of what the test statistics might look like if the null hypothesis were true. Why is that? If the null hypothesis is true, the true difference in probabilities is zero. Thus we expect estimates of the true difference in probabilities to be around zero. This means that the numerator of the test statistic will tend to be around zero, and the entire test statistic will tend to be around zero. All of this is under the assumption that the null hypothesis is true. This distribution is called the **null distribution** and is also described as the distribution of the test statistic *under the null hypothesis*.

If the null hypothesis is true, any rejection of it is an error. Such an error is called a **type I error**, and the probability of making this type of error is used to pick thresholds for decision making in hypothesis testing. The probability of making a type I error is called the **type I error rate**, is denoted by $\alpha$ and can be written in probability notation as:

$$ \alpha = P(\mathrm{reject} \mid H_0) $$

For historical reasons, this rate is typically set to be 0.05: if the null hypothesis is true, then we expect to incorrectly reject the null hypothesis 5% of the time. In other words, if there really is no difference in the probability of adverse outcomes between the two groups, then we can expect to wrongly conclude that there is a difference 5% of the time. This 5% is very commonly adopted but can vary from discipline to discipline depending on desired levels of stringency. Lower values of $\alpha$ are more stringent because we are saying that the probability of a type I error should be lower. Statistical theory tells us how test statistic thresholds correspond to type I error probabilities. For example, with the null distribution we saw above, test statistic thresholds of 2 and 3 correspond to type I error rates of `r 2*pt(2, df = 10, lower.tail = FALSE)` and `r 2*pt(3, df = 10, lower.tail = FALSE)`, illustrated in the picture below.

```{r fig.align="center"}
library(scales)
plot_tails <- function(thresh) {
	plot(x, y, type = "l", main = "Null distribution", xlab = "Test statistic", ylab = "Density")
	abline(v = thresh, col = "red", lty = "dashed", lwd = 2)
	tail_prob <- 0
	if (any(thresh < 0)) {
		neg_thresh <- thresh[thresh < 0]
		xleft <- seq(-5, neg_thresh, 0.01)
		polygon(x = c(xleft, tail(xleft, 1), head(xleft, 1)), y = c(dt(xleft, df = 10), 0, 0), col = "darkorchid")
		tail_prob <- tail_prob + pt(neg_thresh, df = 10, lower.tail = TRUE)
	}
	if (any(thresh > 0)) {
		pos_thresh <- thresh[thresh > 0]
		xright <- seq(pos_thresh, 5, 0.01)
		polygon(x = c(xright, tail(xright, 1), head(xright, 1)), y = c(dt(xright, df = 10), 0, 0), col = "darkorchid")
		tail_prob <- tail_prob + pt(pos_thresh, df = 10, lower.tail = FALSE)
	}
	invisible(tail_prob)
}
par(mfrow = c(1,2))
thresh <- c(-2,2)
tail_prob <- plot_tails(thresh)
text(x = 3.5, y = 0.07, labels = paste("Area =", round(tail_prob, 2)), col = "darkorchid")

thresh <- c(-3,3)
tail_prob <- plot_tails(thresh)
text(x = 3.5, y = 0.07, labels = paste("Area =", round(tail_prob, 2)), col = "darkorchid")
```

A threshold of 2 would not work if we wanted to maintain a type I error rate of 0.05 because it is associated with a higher type I error rate of `r 2*pt(2, df = 10, lower.tail = FALSE)`. It is too low of a threshold because using it would give a higher rate of type I errors than we want. A threshold of 3 is too high of a threshold. While the associated error rates are lower than our 0.05 threshold, using too high of a threshold on the test statistic means that we are also less likely to reject the null when it truly is false. In this example, the threshold that corresponds to $\alpha = 0.05$ is `r round(qt(0.975, df = 10), 2)`.

A very closely related concept is the **p-value**. A p-value is calculated from a particular test statistic and it represents the probability of seeing a test statistic as or more extreme than the one seen if the null hypothesis were true. Let's say that our statistical test gave us a test statistic of $t$, then we can express the p-value as:

$$ \hbox{p-value} = P(\|\hbox{test statistic}\| \geq t \mid H_0) $$

Similarly to how high values of the test statistic indicate higher discrepancy with the null hypothesis, low p-values indicate higher discrepancy with the null hypothesis. Why? Remember that test statistics provide a measure of discrepancy with the null hypothesis. A low p-value tells us that, if the null were true, it would be unlikely to see a test statistic as or more extreme than the one we saw.

We have seen how test statistics and p-values give discrepancy measures with the null hypothesis. We have not focused our attention on looking directly at our estimate of the true difference in adverse event rates. Recall that our estimate of $p_E - p_U$ was 0.35. Surely, the difference in probabilities is not *exactly* 0.35. It would be nice to express this estimate with some sort of "wiggle" room, with some degree of uncertainty. 

### One and two-tailed tests



For a two-tailed test:

$$ H_0: \mathrm{difference} = 0 $$
$$ H_A: \mathrm{difference} \neq 0 $$

```{r}
critvals <- qt(c(0.025, 0.975), df = 10)
plot(x, y, type = "l", main = "Null distribution", xlab = "Test statistic", ylab = "Density")
xleft <- seq(-5, critvals[1], 0.01)
xright <- seq(critvals[2], 5, 0.01)
abline(v = critvals, col = "red", lty = "dashed", lwd = 2)
polygon(x = c(xleft, tail(xleft, 1), head(xleft, 1)), y = c(dt(xleft, df = 10), 0, 0), col = "darkorchid")
polygon(x = c(xright, tail(xright, 1), head(xright, 1)), y = c(dt(xright, df = 10), 0, 0), col = "darkorchid")
text(x = 3.5, y = 0.2, labels = "Area = 0.05", col = "darkorchid")
```




For a one-tailed test:

$$ H_0: \mathrm{difference} = 0 $$
$$ H_A: \mathrm{difference} > 0 $$

```{r}
critval <- qt(0.95, df = 10)
plot(x, y, type = "l", main = "Distribution of test statistic under the null hypothesis", xlab = "Test statistic", ylab = "Density")
xside <- seq(critval, 5, 0.01)
abline(v = critval, col = "red", lty = "dashed", lwd = 2)
polygon(x = c(xside, tail(xside, 1), head(xside, 1)), y = c(dt(xside, df = 10), 0, 0), col = "darkorchid")
text(x = 3.5, y = 0.2, labels = "Area = 0.05", col = "darkorchid")
```




For a one-tailed test:

$$ H_0: \mathrm{difference} = 0 $$
$$ H_A: \mathrm{difference} < 0 $$

```{r}
critval <- qt(0.05, df = 10)
plot(x, y, type = "l", main = "Distribution of test statistic under the null hypothesis", xlab = "Test statistic", ylab = "Density")
xside <- seq(-5, critval, 0.01)
abline(v = critval, col = "red", lty = "dashed", lwd = 2)
polygon(x = c(xside, tail(xside, 1), head(xside, 1)), y = c(dt(xside, df = 10), 0, 0), col = "darkorchid")
text(x = 3.5, y = 0.2, labels = "Area = 0.05", col = "darkorchid")
```





Statistical power is the probability of detecting an association **given that there truly is an association**. While p-values require us to know the distribution of the test statistic "under the null", calculating power requires us to know the distribution "under the alternative".

The gray lines indicate the test statistic thresholds (two-tailed) for rejection for a type I error rate $\alpha = 0.05$. The gray area is equal to to $\alpha$.

```{r}
x <- seq(-5, 8, 0.01)
y <- dt(x, df = 10)
y2 <- dt(x, df = 10, ncp = 1)
plot(x, y, type = "l", main = "Distribution of test statistic", xlab = "Test statistic", ylab = "Density")
lines(x, y2, col = "red")
legend("topright", legend = c("Under null", "Under alternative"), col = c("black", "red"), lty = "solid", bty = "n")
critvals <- qt(c(0.025, 0.975), df = 10)
abline(v = critvals, col = "gray50", lty = "dashed")
xleft <- seq(-5, critvals[1], 0.01)
xright <- seq(critvals[2], 8, 0.01)
polygon(x = c(xleft, tail(xleft, 1), head(xleft, 1)), y = c(dt(xleft, df = 10), 0, 0), col = alpha("gray50", 0.5))
polygon(x = c(xright, tail(xright, 1), head(xright, 1)), y = c(dt(xright, df = 10), 0, 0), col = alpha("gray50", 0.5))
text(x = 5, y = 0.2, labels = paste0("alpha = ", 1-diff(pt(critvals, df = 10))), col = "gray50")
```





Because the thresholds (gray lines) indicate when we reject the null, we can look at the pink area underneath the alternative distribution (red) to calculate power.

```{r}
x <- seq(-5, 8, 0.01)
y <- dt(x, df = 10)
y2 <- dt(x, df = 10, ncp = 1)
plot(x, y, type = "l", main = "Distribution of test statistic", xlab = "Test statistic", ylab = "Density")
lines(x, y2, col = "red")
legend("topright", legend = c("Under null", "Under alternative"), col = c("black", "red"), lty = "solid", bty = "n")
critvals <- qt(c(0.025, 0.975), df = 10)
abline(v = critvals, col = "gray50", lty = "dashed")
xleft <- seq(-5, critvals[1], 0.01)
xright <- seq(critvals[2], 8, 0.01)
polygon(x = c(xleft, tail(xleft, 1), head(xleft, 1)), y = c(dt(xleft, ncp = 1, df = 10), 0, 0), col = alpha("deeppink", 0.5))
polygon(x = c(xright, tail(xright, 1), head(xright, 1)), y = c(dt(xright, ncp = 1, df = 10), 0, 0), col = alpha("deeppink", 0.5))
text(x = 5, y = 0.2, labels = paste0("Power = ", round(1-diff(pt(critvals, ncp = 1, df = 10)), 2)), col = "deeppink")
```







## Common statistical tests

### t-test


### Chi-squared tests


### Nonparametric tests


## Multiple testing
